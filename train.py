# -*- coding: utf-8 -*-
"""Untitled52.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xeegU5aRLavLIyrQ_Gmec5dBd3Dv_Zbw
"""

import os
import shutil
from google.colab import drive

# Mount Drive (no need if already mounted)
drive.mount('/content/drive')

# ‚úÖ Corrected root path
src_root = "/content/drive/MyDrive/Assignment/assignment_dataset/verified_data/verified_data"
dst_root = "/content/train_data"

# Class name mapping
class_map = {
    "barbell biceps curl": "bicep_curl",
    "lateral raise": "lateral_raise",
    "squat": "squat"
}

os.makedirs(dst_root, exist_ok=True)

for subfolder in ["data_btc_10s", "data_crawl_10s"]:
    sub_path = os.path.join(src_root, subfolder)
    if not os.path.exists(sub_path):
        print(f"‚ùå Not found: {sub_path}")
        continue

    for class_name in os.listdir(sub_path):
        full_class_path = os.path.join(sub_path, class_name)
        if class_name.startswith('.') or not os.path.isdir(full_class_path):
            continue  # ‚úÖ skip .DS_Store and any non-directory

        new_class_name = class_map.get(class_name.lower(), class_name.lower())
        dst_class = os.path.join(dst_root, new_class_name)
        os.makedirs(dst_class, exist_ok=True)

        for file in os.listdir(full_class_path):
            src_file = os.path.join(full_class_path, file)
            dst_file = os.path.join(dst_class, f"{subfolder}_{file}")
            if os.path.isfile(src_file):
                shutil.copy2(src_file, dst_file)

print("‚úÖ Final merged dataset created at:", dst_root)

!pip install torchvision decord

import os
import torch
import torchvision
from torchvision import transforms
from torchvision.datasets import DatasetFolder
from torchvision.io import read_video
from torch.utils.data import DataLoader, random_split
from decord import VideoReader, cpu
import random
import numpy as np
from PIL import Image

torch.manual_seed(42)

VIDEO_EXTENSIONS = ['.mp4', '.avi', '.mov']

# Custom loader using decord
def video_loader_decord(path, num_frames=16):
    vr = VideoReader(path, ctx=cpu(0))
    total_frames = len(vr)
    if total_frames < num_frames:
        indices = np.linspace(0, total_frames - 1, num_frames).astype(int)
    else:
        start = random.randint(0, total_frames - num_frames)
        indices = np.arange(start, start + num_frames)
    frames = vr.get_batch(indices).asnumpy()
    frames = [Image.fromarray(frame) for frame in frames]
    return frames

# Frame-wise transform
transform = transforms.Compose([
    transforms.Resize((112, 112)),
    transforms.ToTensor()
])

# Collate to (C, T, H, W)
def collate_fn(batch):
    videos, labels = zip(*batch)
    batch_tensor = []
    for vid in videos:
        vid = [transform(f) for f in vid]
        vid_tensor = torch.stack(vid, dim=1)  # (C, T, H, W)
        batch_tensor.append(vid_tensor)
    return torch.stack(batch_tensor), torch.tensor(labels)

from glob import glob

class GymVideoDataset(torch.utils.data.Dataset):
    def __init__(self, root, classes, num_frames=16):
        self.samples = []
        self.classes = classes
        self.class_to_idx = {cls: i for i, cls in enumerate(classes)}
        self.num_frames = num_frames
        for cls in classes:
            paths = glob(os.path.join(root, cls, "*.mp4"))
            for p in paths:
                self.samples.append((p, self.class_to_idx[cls]))

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        frames = video_loader_decord(path, self.num_frames)
        return frames, label

    def __len__(self):
        return len(self.samples)

# Define classes
classes = ['bicep_curl', 'lateral_raise', 'squat']
dataset = GymVideoDataset("/content/train_data", classes)

# Split
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_ds, val_ds = random_split(dataset, [train_size, val_size])

# Loaders
train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)

import torch.nn as nn
import torch.optim as optim

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = torchvision.models.video.r3d_18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, len(classes))
model = model.to(device)

optimizer = optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

def train_one_epoch():
    model.train()
    total, correct, loss_sum = 0, 0, 0
    for x, y in train_loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        out = model(x)
        loss = criterion(out, y)
        loss.backward()
        optimizer.step()
        pred = out.argmax(dim=1)
        correct += (pred == y).sum().item()
        total += y.size(0)
        loss_sum += loss.item()
    print(f"‚úÖ Train acc: {correct/total:.2f} | Loss: {loss_sum/total:.4f}")

def validate():
    model.eval()
    total, correct = 0, 0
    with torch.no_grad():
        for x, y in val_loader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            pred = out.argmax(dim=1)
            correct += (pred == y).sum().item()
            total += y.size(0)
    print(f"üîç Val acc: {correct/total:.2f}")

# ‚úÖ Step 1: Install dependencies
!pip install torchvision decord

# ‚úÖ Step 2: Imports and setup
import os
import torch
import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader, random_split
from torchvision.io import read_video
from decord import VideoReader, cpu
import random
import numpy as np
from PIL import Image
from glob import glob
import torch.nn as nn
import torch.optim as optim

# Ensure reproducibility
torch.manual_seed(42)

# ‚úÖ Step 3: Video loader function
def video_loader_decord(path, num_frames=16):
    vr = VideoReader(path, ctx=cpu(0))
    total_frames = len(vr)
    if total_frames < num_frames:
        indices = np.linspace(0, total_frames - 1, num_frames).astype(int)
    else:
        start = random.randint(0, total_frames - num_frames)
        indices = np.arange(start, start + num_frames)
    frames = vr.get_batch(indices).asnumpy()
    frames = [Image.fromarray(frame) for frame in frames]
    return frames

# ‚úÖ Step 4: Transform and collate
transform = transforms.Compose([
    transforms.Resize((112, 112)),
    transforms.ToTensor()
])

def collate_fn(batch):
    videos, labels = zip(*batch)
    batch_tensor = []
    for vid in videos:
        vid = [transform(f) for f in vid]
        vid_tensor = torch.stack(vid, dim=1)  # (C, T, H, W)
        batch_tensor.append(vid_tensor)
    return torch.stack(batch_tensor), torch.tensor(labels)

# ‚úÖ Step 5: Dataset class
class GymVideoDataset(torch.utils.data.Dataset):
    def __init__(self, root, classes, num_frames=16):
        self.samples = []
        self.classes = classes
        self.class_to_idx = {cls: i for i, cls in enumerate(classes)}
        self.num_frames = num_frames
        for cls in classes:
            paths = glob(os.path.join(root, cls, "*.mp4"))
            for p in paths:
                self.samples.append((p, self.class_to_idx[cls]))

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        frames = video_loader_decord(path, self.num_frames)
        return frames, label

    def __len__(self):
        return len(self.samples)

# ‚úÖ Step 6: Load dataset
data_path = "/content/train_data"
classes = ['bicep_curl', 'lateral_raise', 'squat']
dataset = GymVideoDataset(data_path, classes)

train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_ds, val_ds = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)

# ‚úÖ Step 7: Define model and training functions
from torchvision.models.video import r3d_18, R3D_18_Weights
weights = R3D_18_Weights.DEFAULT

model = r3d_18(weights=weights)
model.fc = nn.Linear(model.fc.in_features, len(classes))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

optimizer = optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

def train_one_epoch():
    model.train()
    total, correct, loss_sum = 0, 0, 0
    for x, y in train_loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        out = model(x)
        loss = criterion(out, y)
        loss.backward()
        optimizer.step()
        pred = out.argmax(dim=1)
        correct += (pred == y).sum().item()
        total += y.size(0)
        loss_sum += loss.item()
    print("Train acc: {:.2f} | Loss: {:.4f}".format(correct/total, loss_sum/total))

def validate():
    model.eval()
    total, correct = 0, 0
    with torch.no_grad():
        for x, y in val_loader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            pred = out.argmax(dim=1)
            correct += (pred == y).sum().item()
            total += y.size(0)
    print("Val acc: {:.2f}".format(correct/total))

# ‚úÖ Step 8: Train
for epoch in range(5):
    print("Epoch {}".format(epoch+1))
    train_one_epoch()
    validate()

# ‚úÖ Step 9: Inference on single video
def predict_video(path):
    model.eval()
    frames = video_loader_decord(path)
    frames = [transform(f) for f in frames]
    x = torch.stack(frames, dim=1).unsqueeze(0).to(device)  # (1, C, T, H, W)
    with torch.no_grad():
        out = model(x)
        pred = out.argmax(dim=1).item()
    print("Prediction: {}".format(classes[pred]))

# Example usage:
# predict_video("/content/train_data/squat/your_video.mp4")

# ‚úÖ Step 1: Install dependencies
!pip install torchvision decord tqdm

# ‚úÖ Step 2: Imports and setup
import os
import torch
import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader, random_split
from torchvision.io import read_video
from decord import VideoReader, cpu
import random
import numpy as np
from PIL import Image
from glob import glob
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm

# Ensure reproducibility
torch.manual_seed(42)

# ‚úÖ Step 3: Video loader function
def video_loader_decord(path, num_frames=16):
    vr = VideoReader(path, ctx=cpu(0))
    total_frames = len(vr)
    if total_frames < num_frames:
        indices = np.linspace(0, total_frames - 1, num_frames).astype(int)
    else:
        start = random.randint(0, total_frames - num_frames)
        indices = np.arange(start, start + num_frames)
    frames = vr.get_batch(indices).asnumpy()
    frames = [Image.fromarray(frame) for frame in frames]
    return frames

# ‚úÖ Step 4: Transform and collate
transform = transforms.Compose([
    transforms.Resize((112, 112)),
    transforms.ToTensor()
])

def collate_fn(batch):
    videos, labels = zip(*batch)
    batch_tensor = []
    for vid in videos:
        vid = [transform(f) for f in vid]
        vid_tensor = torch.stack(vid, dim=1)  # (C, T, H, W)
        batch_tensor.append(vid_tensor)
    return torch.stack(batch_tensor), torch.tensor(labels)

# ‚úÖ Step 5: Dataset class
class GymVideoDataset(torch.utils.data.Dataset):
    def __init__(self, root, classes, num_frames=16):
        self.samples = []
        self.classes = classes
        self.class_to_idx = {cls: i for i, cls in enumerate(classes)}
        self.num_frames = num_frames
        for cls in classes:
            paths = glob(os.path.join(root, cls, "*.mp4"))
            for p in paths:
                self.samples.append((p, self.class_to_idx[cls]))

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        frames = video_loader_decord(path, self.num_frames)
        return frames, label

    def __len__(self):
        return len(self.samples)

# ‚úÖ Step 6: Load dataset
data_path = "/content/train_data"
classes = ['bicep_curl', 'lateral_raise', 'squat']
dataset = GymVideoDataset(data_path, classes)

train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_ds, val_ds = random_split(dataset, [train_size, val_size])

# Optionally reduce dataset for faster debugging
# train_ds, _ = random_split(train_ds, [int(0.2 * len(train_ds)), len(train_ds) - int(0.2 * len(train_ds))])

train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=2)
val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=2)

print("Total videos found:", len(dataset))

# ‚úÖ Step 7: Define model and training functions
from torchvision.models.video import r3d_18, R3D_18_Weights
weights = R3D_18_Weights.DEFAULT

model = r3d_18(weights=weights)
model.fc = nn.Linear(model.fc.in_features, len(classes))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)
model = model.to(device)

optimizer = optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

def train_one_epoch():
    model.train()
    total, correct, loss_sum = 0, 0, 0
    for x, y in tqdm(train_loader, desc="Training"):
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        out = model(x)
        loss = criterion(out, y)
        loss.backward()
        optimizer.step()
        pred = out.argmax(dim=1)
        correct += (pred == y).sum().item()
        total += y.size(0)
        loss_sum += loss.item()
    print("Train acc: {:.2f} | Loss: {:.4f}".format(correct/total, loss_sum/total))

def validate():
    model.eval()
    total, correct = 0, 0
    with torch.no_grad():
        for x, y in tqdm(val_loader, desc="Validation"):
            x, y = x.to(device), y.to(device)
            out = model(x)
            pred = out.argmax(dim=1)
            correct += (pred == y).sum().item()
            total += y.size(0)
    print("Val acc: {:.2f}".format(correct/total))

# ‚úÖ Step 1: Install dependencies
!pip install torchvision decord tqdm

# ‚úÖ Step 2: Imports and setup
import os
import torch
import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader, random_split
from torchvision.io import read_video
from decord import VideoReader, cpu
import random
import numpy as np
from PIL import Image
from glob import glob
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm

# Ensure reproducibility
torch.manual_seed(42)

# ‚úÖ Step 3: Video loader function
def video_loader_decord(path, num_frames=16):
    vr = VideoReader(path, ctx=cpu(0))
    total_frames = len(vr)
    if total_frames < num_frames:
        indices = np.linspace(0, total_frames - 1, num_frames).astype(int)
    else:
        start = random.randint(0, total_frames - num_frames)
        indices = np.arange(start, start + num_frames)
    frames = vr.get_batch(indices).asnumpy()
    frames = [Image.fromarray(frame) for frame in frames]
    return frames

# ‚úÖ Step 4: Transform and collate
transform = transforms.Compose([
    transforms.Resize((112, 112)),
    transforms.ToTensor()
])

def collate_fn(batch):
    videos, labels = zip(*batch)
    batch_tensor = []
    for vid in videos:
        vid = [transform(f) for f in vid]
        vid_tensor = torch.stack(vid, dim=1)  # (C, T, H, W)
        batch_tensor.append(vid_tensor)
    return torch.stack(batch_tensor), torch.tensor(labels)

# ‚úÖ Step 5: Dataset class
class GymVideoDataset(torch.utils.data.Dataset):
    def __init__(self, root, classes, num_frames=16):
        self.samples = []
        self.classes = classes
        self.class_to_idx = {cls: i for i, cls in enumerate(classes)}
        self.num_frames = num_frames
        for cls in classes:
            paths = glob(os.path.join(root, cls, "*.mp4"))
            for p in paths:
                self.samples.append((p, self.class_to_idx[cls]))

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        frames = video_loader_decord(path, self.num_frames)
        return frames, label

    def __len__(self):
        return len(self.samples)

# ‚úÖ Step 6: Load dataset
data_path = "/content/train_data"
classes = ['bicep_curl', 'lateral_raise', 'squat']
dataset = GymVideoDataset(data_path, classes)

train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_ds, val_ds = random_split(dataset, [train_size, val_size])

# Optionally reduce dataset for faster debugging
# train_ds, _ = random_split(train_ds, [int(0.2 * len(train_ds)), len(train_ds) - int(0.2 * len(train_ds))])

train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=2)
val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=2)

# ‚úÖ Step 7: Define model and training functions
from torchvision.models.video import r3d_18, R3D_18_Weights
weights = R3D_18_Weights.DEFAULT

model = r3d_18(weights=weights)
model.fc = nn.Linear(model.fc.in_features, len(classes))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)
model = model.to(device)

optimizer = optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

def train_one_epoch():
    model.train()
    total, correct, loss_sum = 0, 0, 0
    for x, y in tqdm(train_loader, desc="Training"):
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        out = model(x)
        loss = criterion(out, y)
        loss.backward()
        optimizer.step()
        pred = out.argmax(dim=1)
        correct += (pred == y).sum().item()
        total += y.size(0)
        loss_sum += loss.item()
    acc = correct / total
    avg_loss = loss_sum / total
    return acc, avg_loss

def validate():
    model.eval()
    total, correct = 0, 0
    with torch.no_grad():
        for x, y in tqdm(val_loader, desc="Validation"):
            x, y = x.to(device), y.to(device)
            out = model(x)
            pred = out.argmax(dim=1)
            correct += (pred == y).sum().item()
            total += y.size(0)
    acc = correct / total
    return acc

# ‚úÖ Step 8: Train
for epoch in range(3):  # Use 3 epochs for faster testing
    print("\nEpoch {}".format(epoch+1))
    train_acc, train_loss = train_one_epoch()
    val_acc = validate()
    print("Epoch {} Summary: Train Acc: {:.2f}, Train Loss: {:.4f}, Val Acc: {:.2f}".format(epoch+1, train_acc, train_loss, val_acc))

# ‚úÖ Step 9: Inference on single video
def predict_video(path):
    model.eval()
    frames = video_loader_decord(path)
    frames = [transform(f) for f in frames]
    x = torch.stack(frames, dim=1).unsqueeze(0).to(device)  # (1, C, T, H, W)
    with torch.no_grad():
        out = model(x)
        pred = out.argmax(dim=1).item()
    print("Prediction: {}".format(classes[pred]))

# Example usage:
# predict_video("/content/train_data/squat/example.mp4")

# ‚úÖ Step 9: Inference on single video
def predict_video(path):
    model.eval()
    frames = video_loader_decord(path)
    frames = [transform(f) for f in frames]
    x = torch.stack(frames, dim=1).unsqueeze(0).to(device)  # (1, C, T, H, W)
    with torch.no_grad():
        out = model(x)
        pred = out.argmax(dim=1).item()
    print("Prediction: {}".format(classes[pred]))

# Example usage:
predict_video("/content/drive/MyDrive/Assignment/assignment_dataset/verified_data/verified_data/data_btc_10s/barbell biceps curl/01502df8-593c-42d0-86ae-7cbd48d8741a.mp4")

predict_video("/content/drive/MyDrive/Assignment/assignment_dataset/verified_data/verified_data/data_btc_10s/squat/ed572c9b-ca11-4d01-baed-bea0e631179e.mp4")

torch.save(model.state_dict(), "model.pth")

from google.colab import files
files.download("model.pth")

